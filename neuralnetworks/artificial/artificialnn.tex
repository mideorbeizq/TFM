%\section{Artificial Neural Networks}
\label{sec:ANN}
ANNs represent a type of computing based on the way that the brain performs computations. It does not approach the complexity of the brain; however, there are three similarities between biological and artificial neural networks: 

\begin{itemize}
	\item The basic unit of both systems -the neuron- is a very simple computational element that are highly interconnected.
	\item The connections among those neurons determine the function of the network.
	\item Despite having a very simple set of rules of behaviour, when a neuron interacts with others, the global response of the system becomes much more complex.
\end{itemize}

The next sections try to explain how a biological neuron is model in order to build artificial neural networks capable of solving complex problems.

\subsection{Neuron model}
\label{subsec:neuronmodel}

As described in \secref{BNN}, a typical biological neuron has four main regions: the soma, the axon, the dendrites and the synapses. These parts are also be differentiated in an artificial neuron. \figref{neuronmodel} shows how a biological neuron is model for ANNs.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{images/neuronmodel.png}
\caption{Model of an artificial neuron inspired in a biological one}
\label{fig:neuronmodel}
\end{figure}

In an artificial neuron, the \emph{inputs} $p_{k}, k=1,...,R$ are multiplied by \emph{weights} $w_{k}$ and summed up together with the optional constant \emph{bias} or \emph{offset} term $b$. The resulting scalar $n$, often referred to as the \emph{net input}, goes into the \emph{activation function} -also called \emph{transfer function}- $f$, which produces the scalar neuron \emph{output} $a$.
Thus, the \emph{output} of the neuron $i$ becomes \cite{Demuth:2014:NND:2721661}:

\begin{equation}
a_{i}=f_{i}(n_{i})=f_{i}(\sum_{k=1}^R w_{ki}p_{k}+b_{i})
\label{eq:expandedgeneralneuroneq}
\end{equation}

Relating this neuron model back to the biological neuron that we discussed in \secref{BNN}, the set of \emph{inputs} $p_{k}$ correspond to the dendritic tree,
each \emph{weight} $w_{k}$ emulates the strength of a synapse, the soma is represented by the summation and the \emph{activation function} $f$,
and the neuron \emph{output} $y$ represents the signal on the axon.

The behave of the neuron depends strongly on the particular activation function that is chosen. Then the scalar parameters $w_{k}$ and $b$ will be adjusted by some learning rule so that the neuron input/output relationship meets some specific goal.

The activation function $f$ may be a linear or a nonlinear function of the net input $n$. 
A particular function is chosen to satisfy some specification of the problem that the neuron is attempting to solve. The following are some of the functions often used in ANNs \cite{Demuth:2014:NND:2721661}:

\begin{description}
	\item{\textbf{\emph{Hard limit} or \emph{Step} function}\hfill \\
	It sets the output of the neuron to 0 if the function argument is less than 0, or 1 if its argument is greater than or equal to 0. Its symetric version follows the expression:
		\begin{equation}
		a = f(n) =
  			\begin{cases}
    			-1 & \text{if } n < 0\\
    			1 & \text{if } n \geq 0
  			\end{cases}
		\label{eq:hardlimit}
		\end{equation}

	This activation function is commonly used to create neurons that classify inputs into two distinct categories.
	}
	\item{\textbf{\emph{Linear} function}\hfill \\
	The output of a linear transfer function is equal to its input ($a=f(n)=n$). This simple function have some variations that are often used in artificial neurons:
	\begin{itemize}
		\item \emph{Positive linear} function, which guarantees to take nonnegative values according to the equation:
		\begin{equation}
		a = f(n) =
  			\begin{cases}
    			0 & \text{if } n < 0\\
    			n & \text{if } n \geq 0
  			\end{cases}
		\label{eq:positivelinear}
		\end{equation}

		\item \emph{Saturating linear} function, whose symmetric version is mathematically expressed as:
		\begin{equation}
		a = f(n) =
  			\begin{cases}
    			-1 & \text{if } n < -1\\
    			n & \text{if } -1 \leq n \leq 1\\
    			1 & \text{if } n > 1
  			\end{cases}
		\label{eq:saturatinglinear}
		\end{equation}
	\end{itemize}
	}
	\item{\textbf{\emph{Log-Sigmoid} function}\hfill \\
	This activation function takes the input (which may have any value between plus and minus infinity) and squashes the output into the range 0 to 1, according to the expression:

	\begin{equation}
	a = f(n) = \frac{1}{1+e^{-n}}
	\label{eq:logsig}
	\end{equation}

	The log-sigmoid function is commonly used in multilayer networks that are trained using the backpropagation learning rule (see \secref{learningrules}), in part because this function is differentiable.
	}
	\item{\textbf{\emph{Hyperbolic Tangent Sigmoid} function}\hfill \\
	The hyperbolic tangent function produces positive numbers between -1 and 1 according to:
	\begin{equation}
	a = f(n) = \frac{e^{n}-e^{-n}}{e^{n}+e^{-n}}
	\label{eq:tansig}
	\end{equation}
	Because this function has a derivative, it can also be used with gradient descent based training methods as it is the backpropagation rule.
	}

\end{description}

\subsection{Neural network model}
\label{subsec:neuralnetworkmodel}

An ANN is then build by connecting several neurons.
Serial connections comprise different layers of neurons. 
Parallel connections increase the number of neurons in the same layer of the network.
The number of neurons in the last layer determines the number of outputs $S$ of the network. Thus, a huge amount of different topologies can be form. 

In addition to the activation function $f$, the number of inputs $R$ of each neuron can be configured, resulting in almost an infinite source of possible combinations with which try to solve any kind of scenario.
It must be noted that the activation functions are usually assumed to be the same in each layer of neurons. 
Normally a hyberbolic tangent or the sigmoid function is used, leading the ANN to a nonlinear parameterized map from input space $p\in\mathbb{R}^S$ to output space 
$a\in\mathbb{R}^R$.

\figref{neuralnetworkmodel} shows an example of an artificial neural network topology with three layers and the same number of neurons in each layer. 

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{images/neuralnetworkmodel.png}
\caption{Model of three-layer Artificial Neural Network}
\label{fig:neuralnetworkmodel}
\end{figure}

Denoting
$\mathbf{p}$ as the $R\times 1$-dimensional input vector of the whole network, 
$\mathbf{b}^1$ the $S\times 1$-dimensional bias vector of the first layer and 
$\mathbf{W}^1$ as the $S\times R$-dimensional matrix of weights of the same layer, 
the $S\times 1$-dimensional net input vector of that particular layer $\mathbf{n}^1$ can be expressed as:
\begin{equation} 
\mathbf{n}^1=\mathbf{W}^1\mathbf{p}+\mathbf{b}^1
\label{eq:netinputvector}
\end{equation}

This way, \figref{neuralnetworkmodel} can we redrawn as \figref{neuralnetworkmodelVectorial} and, combining \esref{expandedgeneralneuroneq}{netinputvector}, the $S\times 1$-dimensional output vector of the whole network 
$\mathbf{a}^3$ can be written as:

\begin{equation}
\mathbf{a}^3=\mathbf{f}^3(\mathbf{W}^3\mathbf{f}^2(\mathbf{W}^2\mathbf{f}^1(\mathbf{W}^1\mathbf{p}+\mathbf{b}^1)+\mathbf{b}^2)+\mathbf{b}^3)
\label{eq:ouputnetworkvector}
\end{equation}

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{images/neuralnetworkmodelVectorial.png}
\caption{Model of three-layer Artificial Neural Network with matrix notation}
\label{fig:neuralnetworkmodelVectorial}
\end{figure}

It is important to highlight that, when implementing ANNs, the first layer is called \emph{input layer}, the last one \emph{output layer} and all the intermediate ones are known as \emph{hidden layers}. When there is no hidden layers, it is said that the net is a one-layer network. This way, two-layers networks will denote a net with a unique hidden layer. The reason of this is that the weigths of the input layer are usually fixed to $w=1$.

\subsection{Advantages and disadvantages}

\textsc{COPY-PASTE!!!!!!!!!!!!!!!!!!!!!!!!!!!! - ESCRIBIR}\\
Due to the admissible predictive performance of NN, it is presently the
most popular data modeling method used in the medical domain [37]. The ability of the NN is to model
highly nonlinear systems such as physiological records where the correlation of the input parameters is
not easily detectable [71].

In sum, since the progress of learning in NN would be complex, the method is commonly used for
decision making in clinical conditions with large and complicated data sets. But same as SVM it could
not handle domain knowledge to enrich the results. Additionally, as the modeling process in NN is a
black box progress, NN method needs to justify for each input data. So, NN is not counted as a portable
technique to easily apply for diverse data sets.

Artificial neural networks were up until recently the
most popular artificial intelligence-based data modeling
algorithm used in clinical medicine. This is probably due to
their good predictive performance, albeit they may have a
number of deficiencies [18] that include high sensitivity to the
parameters of the method—including those that determine
the architecture of the network, high computational cost in
training, and induction of the model that may – at best – be
hard to interpret by domain experts. Neural networks may be
able to model complex non-linear relationships, comprising
an advantage over simpler modeling methods like the na ̈ve
ı
Bayesian classifier or logistic regression.
}
